{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rezamohammadloo/Surface_anomaly_detection_using_unsupervised_algorithms/blob/main/_Surface_Defect_Detection_Unsupervised.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-zig6s0uMPe"
      },
      "source": [
        "Hi, this is a project that has been done to implement some unsupervised algorhimgs for \"Anomaly Detection\" or \"Surface Anomaly Detection\" problem on an image dataset with image sizes: 128*128*3.\n",
        "4 different methodes are implemented for this purpose and the result of them are evaluated by AUROC and AP techniques to identify the best one.\n",
        "The first algorithm is a VAE (Varitional Autoencoer)\n",
        "the second one is a regular autoencoder.\n",
        "the third one is OCSVM(One-Class Support Vector Machine)\n",
        "and finally the last algorithm is Isolation Forest.\n",
        "Befor designing and trainin model some of the main libararies and packages have been implemented to use and some functions and variables that are in common between methodes are defined.\n",
        "Let's dive in!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pHA9PY7DWYb5",
        "outputId": "b29806c1-6235-4af2-f15b-f95c744726f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Mounting Google Drive to access the dataset stored in my Google Drive account\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "tixxm-fAYGDh"
      },
      "outputs": [],
      "source": [
        "# Importing necessary libraries for building and training models\n",
        "import os  # For operating system dependent functionality\n",
        "import numpy as np  # For numerical operations and array manipulations\n",
        "import tensorflow as tf  # For deep learning model creation and training\n",
        "import matplotlib.pyplot as plt  # For plotting and visualizing data\n",
        "from sklearn.svm import OneClassSVM  # For One-Class SVM, a traditional anomaly detection method\n",
        "from sklearn.pipeline import Pipeline  # For creating pipelines that streamline preprocessing and model training\n",
        "from tensorflow.keras.models import Model  # For building Keras models\n",
        "from sklearn.ensemble import IsolationForest  # For Isolation Forest, another traditional anomaly detection method\n",
        "from sklearn.preprocessing import StandardScaler  # For standardizing features by removing the mean and scaling to unit variance\n",
        "from sklearn.model_selection import GridSearchCV  # For performing hyperparameter tuning using grid search\n",
        "from tensorflow.keras.callbacks import EarlyStopping  # For stopping training early if the model stops improving\n",
        "from sklearn.model_selection import train_test_split  # For splitting data into training and testing sets\n",
        "from tensorflow.keras import layers, models, backend as K  # For creating layers, models, and accessing Keras backend\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score  # For evaluating model performance using ROC AUC and average precision\n",
        "from tensorflow.keras.preprocessing.image import img_to_array, load_img  # For converting images to arrays and loading images\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D  # For creating convolutional autoencoders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ogI-pnC0m2oS"
      },
      "source": [
        "#0.General Functions and parameters\n",
        "There are some finctions and varaiables that will be used in some or all of the methods.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "loHE_e8srCst"
      },
      "outputs": [],
      "source": [
        "# Define a function to load images from a specified directories\n",
        "def load_images_from_directory(directory, target_size=(128, 128)):\n",
        "    images = []\n",
        "    for filename in os.listdir(directory):\n",
        "        if filename.endswith('.png') or filename.endswith('.jpg'):\n",
        "            img = load_img(os.path.join(directory, filename), target_size=target_size)\n",
        "            img_array = img_to_array(img) #/ 255.0  #Deviding by 255 in canceled here because we don't need to do it for third and furth method and we'll do it in the first and second method by hand.\n",
        "            images.append(img_array)\n",
        "    return np.array(images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "JuI59Jwrtfdc"
      },
      "outputs": [],
      "source": [
        "# Directories\n",
        "# Specify the directories containing normal, anomaly, and mask images\n",
        "normal_dir  = '/content/gdrive/MyDrive/glass-defect-sample/good'\n",
        "anomaly_dir = '/content/gdrive/MyDrive/glass-defect-sample/anomaly'\n",
        "mask_dir    = '/content/gdrive/MyDrive/glass-defect-sample/mask'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "mE1_M4Hkr1e8"
      },
      "outputs": [],
      "source": [
        "# Flatten images to vectors, it will be used for third and furth methodes\n",
        "def flatten_images(images):\n",
        "    return images.reshape(images.shape[0], -1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "zFcJ-Tjl0gDg"
      },
      "outputs": [],
      "source": [
        "# Define a function to compute reconstruction errors.\n",
        "def compute_reconstruction_error(original, reconstructed):\n",
        "    return np.mean(np.abs(original - reconstructed), axis=(1, 2, 3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ufu9Hy2iYGBa",
        "outputId": "13b371fb-c3a8-4a95-9390-980fe2383ed0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Normal Images Shape: (157, 128, 128, 3)\n",
            "Anomaly Images Shape: (40, 128, 128, 3)\n",
            "Mask Images Shape: (40, 128, 128, 3)\n"
          ]
        }
      ],
      "source": [
        "# Load data\n",
        "# Load the normal, anomaly, and mask images from their respective directories\n",
        "normal_images  = load_images_from_directory(normal_dir)\n",
        "anomaly_images = load_images_from_directory(anomaly_dir)\n",
        "mask_images    = load_images_from_directory( mask_dir )\n",
        "\n",
        "# Print the shape of the loaded images\n",
        "print(\"Normal Images Shape:\", normal_images.shape)\n",
        "print(\"Anomaly Images Shape:\", anomaly_images.shape)\n",
        "print(\"Mask Images Shape:\", mask_images.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NdayKebpY88a"
      },
      "source": [
        "#1.Varational Autoencoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CojbIrUcyWqc"
      },
      "source": [
        "First we normalize loaded images to make the most use out of them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "UsHT9Ewktqxe"
      },
      "outputs": [],
      "source": [
        "#Normalizing their values to [0, 1]\n",
        "normal_images  =  normal_images / 255.0\n",
        "anomaly_images =  anomaly_images / 255.0\n",
        "mask_images    =  mask_images / 255.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b94RfoFF8pVj"
      },
      "source": [
        "Now it's time to design the architecture of VAE."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "AGkSYlXFZOdZ"
      },
      "outputs": [],
      "source": [
        "latent_dim = 32 #Dimensionality of the latent space for the Variational Autoencoder (VAE)\n",
        "\n",
        "# Encoder model definition\n",
        "def encoder_model(input_shape):\n",
        "    # Define input layer\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "\n",
        "    # Convolutional layers with ReLU activation and same padding\n",
        "    x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n",
        "    x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
        "\n",
        "    x = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
        "\n",
        "    x = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
        "\n",
        "    # Flatten the output and pass through dense layers to get z_mean and z_log_var\n",
        "    x = layers.Flatten()(x)\n",
        "    x = layers.Dense(128, activation='relu')(x)\n",
        "    z_mean = layers.Dense(latent_dim)(x)      # Mean of the latent space\n",
        "    z_log_var = layers.Dense(latent_dim)(x)   # Log variance of the latent space\n",
        "\n",
        "    # Define and return the encoder model\n",
        "    return models.Model(inputs, [z_mean, z_log_var], name='encoder')\n",
        "\n",
        "# Sampling function definition for the VAE\n",
        "def sampling(args):\n",
        "    # Retrieve z_mean and z_log_var from arguments\n",
        "    z_mean, z_log_var = args\n",
        "\n",
        "    # Determine batch size and dimensionality of z_mean\n",
        "    batch = K.shape(z_mean)[0]\n",
        "    dim = K.int_shape(z_mean)[1]\n",
        "\n",
        "    # Sample epsilon from normal distribution\n",
        "    epsilon = K.random_normal(shape=(batch, dim))\n",
        "\n",
        "    # Reparameterization trick: sample from the learned distribution\n",
        "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
        "\n",
        "# Decoder model definition\n",
        "def decoder_model(latent_dim, input_shape):\n",
        "    # Define input layer for latent space\n",
        "    latent_inputs = layers.Input(shape=(latent_dim,))\n",
        "\n",
        "    # Dense layer to reshape and feed into convolutional layers\n",
        "    x = layers.Dense((input_shape[0] // 8) * (input_shape[1] // 8) * 128, activation='relu')(latent_inputs)\n",
        "    x = layers.Reshape((input_shape[0] // 8, input_shape[1] // 8, 128))(x)\n",
        "\n",
        "    # Convolutional transpose layers for upsampling and decoding\n",
        "    x = layers.Conv2DTranspose(128, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = layers.UpSampling2D((2, 2))(x)\n",
        "\n",
        "    x = layers.Conv2DTranspose(64, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = layers.UpSampling2D((2, 2))(x)\n",
        "\n",
        "    x = layers.Conv2DTranspose(32, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = layers.UpSampling2D((2, 2))(x)\n",
        "\n",
        "    # Output layer with sigmoid activation for image reconstruction\n",
        "    outputs = layers.Conv2DTranspose(3, (3, 3), activation='sigmoid', padding='same')(x)\n",
        "\n",
        "    # Define and return the decoder model\n",
        "    return models.Model(latent_inputs, outputs, name='decoder')\n",
        "\n",
        "# Define input shape for the VAE\n",
        "input_shape = (128, 128, 3)\n",
        "\n",
        "# Build the encoder model\n",
        "encoder = encoder_model(input_shape)\n",
        "z_mean, z_log_var = encoder.output\n",
        "\n",
        "# Use the sampling function to obtain latent space representation\n",
        "z = layers.Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n",
        "\n",
        "# Build the decoder model using the latent space representation\n",
        "decoder = decoder_model(latent_dim, input_shape)\n",
        "\n",
        "# Connect encoder input to decoder output to create VAE model\n",
        "outputs = decoder(z)\n",
        "\n",
        "# Define the VAE model with encoder input and decoder output\n",
        "vae = models.Model(encoder.input, outputs, name='vae')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zS1t100188hH"
      },
      "source": [
        "For traing this model we need to combine two different loss funcions: \"binary cross entropy\" and \"KL-divergence\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "L3E483f0ZW8k"
      },
      "outputs": [],
      "source": [
        "# Calculate the reconstruction loss using binary cross-entropy\n",
        "reconstruction_loss = tf.keras.losses.binary_crossentropy(K.flatten(encoder.input), K.flatten(outputs))\n",
        "# Scale the reconstruction loss based on the input shape\n",
        "reconstruction_loss *= input_shape[0] * input_shape[1] * input_shape[2]\n",
        "\n",
        "# Calculate the KL divergence loss\n",
        "kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n",
        "kl_loss = K.sum(kl_loss, axis=-1)\n",
        "kl_loss *= -0.5\n",
        "\n",
        "# Calculate the total VAE loss by combining the reconstruction and KL divergence losses\n",
        "vae_loss = K.mean(reconstruction_loss + kl_loss)\n",
        "\n",
        "# Add the VAE loss to the model\n",
        "vae.add_loss(vae_loss)\n",
        "\n",
        "# Compile the VAE model with the Adam optimizer\n",
        "vae.compile(optimizer='adam')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "1S_XdkowZevD",
        "outputId": "24f94336-8a06-40a3-d448-25ce63b7f12c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "20/20 [==============================] - 19s 651ms/step - loss: 26085.1445\n",
            "Epoch 2/500\n",
            "20/20 [==============================] - 14s 707ms/step - loss: 23985.5586\n",
            "Epoch 3/500\n",
            "20/20 [==============================] - 14s 716ms/step - loss: 24048.7891\n",
            "Epoch 4/500\n",
            "20/20 [==============================] - 14s 683ms/step - loss: 23985.5566\n",
            "Epoch 5/500\n",
            "20/20 [==============================] - 12s 623ms/step - loss: 23622.1035\n",
            "Epoch 6/500\n",
            "20/20 [==============================] - 12s 597ms/step - loss: 23625.8945\n",
            "Epoch 7/500\n",
            "20/20 [==============================] - 14s 694ms/step - loss: 23751.9531\n",
            "Epoch 8/500\n",
            "20/20 [==============================] - 14s 715ms/step - loss: 23526.6582\n",
            "Epoch 9/500\n",
            "20/20 [==============================] - 14s 713ms/step - loss: 23467.2109\n",
            "Epoch 10/500\n",
            "20/20 [==============================] - 14s 719ms/step - loss: 23455.9727\n",
            "Epoch 11/500\n",
            "20/20 [==============================] - 12s 606ms/step - loss: 23374.2988\n",
            "Epoch 12/500\n",
            "20/20 [==============================] - 14s 681ms/step - loss: 23391.7285\n",
            "Epoch 13/500\n",
            "20/20 [==============================] - 16s 806ms/step - loss: 23417.2148\n",
            "Epoch 14/500\n",
            "20/20 [==============================] - 14s 711ms/step - loss: 23353.7324\n",
            "Epoch 15/500\n",
            "20/20 [==============================] - 14s 704ms/step - loss: 23192.3320\n",
            "Epoch 16/500\n",
            "20/20 [==============================] - 13s 624ms/step - loss: 23213.5293\n",
            "Epoch 17/500\n",
            "20/20 [==============================] - 12s 605ms/step - loss: 23154.4277\n",
            "Epoch 18/500\n",
            "20/20 [==============================] - 14s 701ms/step - loss: 23190.8086\n",
            "Epoch 19/500\n",
            "20/20 [==============================] - 14s 696ms/step - loss: 23185.4219\n",
            "Epoch 20/500\n",
            "20/20 [==============================] - 14s 706ms/step - loss: 23420.1992\n",
            "Epoch 21/500\n",
            "20/20 [==============================] - 13s 667ms/step - loss: 23228.2637\n",
            "Epoch 22/500\n",
            "20/20 [==============================] - 12s 581ms/step - loss: 23151.7617\n",
            "Epoch 23/500\n",
            "20/20 [==============================] - 17s 852ms/step - loss: 23226.2559\n",
            "Epoch 24/500\n",
            "20/20 [==============================] - 14s 698ms/step - loss: 23193.9883\n",
            "Epoch 25/500\n",
            "20/20 [==============================] - 15s 768ms/step - loss: 23064.7324\n",
            "Epoch 26/500\n",
            "20/20 [==============================] - 14s 694ms/step - loss: 23255.2148\n",
            "Epoch 27/500\n",
            "20/20 [==============================] - 14s 684ms/step - loss: 23142.9551\n",
            "Epoch 28/500\n",
            "20/20 [==============================] - 12s 582ms/step - loss: 22992.9648\n",
            "Epoch 29/500\n",
            "20/20 [==============================] - 12s 606ms/step - loss: 23029.0879\n",
            "Epoch 30/500\n",
            "20/20 [==============================] - 14s 691ms/step - loss: 23084.4590\n",
            "Epoch 31/500\n",
            "20/20 [==============================] - 14s 692ms/step - loss: 23014.7168\n",
            "Epoch 32/500\n",
            "20/20 [==============================] - 13s 671ms/step - loss: 23002.8418\n",
            "Epoch 33/500\n",
            "20/20 [==============================] - 11s 544ms/step - loss: 23026.4023\n",
            "Epoch 34/500\n",
            "20/20 [==============================] - ETA: 0s - loss: 23009.9062Restoring model weights from the end of the best epoch: 28.\n",
            "20/20 [==============================] - 13s 661ms/step - loss: 23009.9062\n",
            "Epoch 34: early stopping\n"
          ]
        }
      ],
      "source": [
        "# Define the history object to store training metrics\n",
        "history = vae.fit(\n",
        "    # Train the VAE model on normal images with normal images as both input and target\n",
        "    normal_images, normal_images,\n",
        "    epochs=500,  # Number of epochs for training\n",
        "    batch_size=8,  # Batch size for training\n",
        "    #validation_split=0.2,  # Fraction of training data to use for validation\n",
        "    callbacks=EarlyStopping(  # Early stopping callback to prevent overfitting\n",
        "        monitor=\"loss\",  # Monitor validation loss\n",
        "        verbose=1,  # Verbosity mode (1: progress bar, 0: silent)\n",
        "        patience=6,  # Number of epochs with no improvement after which training will be stopped\n",
        "        mode='min',  # Direction of improvement (minimize validation loss)\n",
        "        restore_best_weights=True  # Restore the model weights from the epoch with the best validation loss\n",
        "    )\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWiOOuQs99Z9"
      },
      "source": [
        "Reconstructing images using designed model in order to determice threshold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "_ZR7pb_kaNhw",
        "outputId": "a88d3c1a-b79a-4c3a-a3aa-016cf6d5b4fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5/5 [==============================] - 4s 678ms/step\n",
            "2/2 [==============================] - 1s 159ms/step\n",
            "Mean  reconstruction  error  for normal  images : 0.021452293\n",
            "Mean reconstruction error for anomalous images: 0.09324009\n"
          ]
        }
      ],
      "source": [
        "# Reconstruct normal and anomaly images using the trained VAE model\n",
        "reconstructed_normal  = vae.predict(normal_images)\n",
        "reconstructed_anomaly = vae.predict(anomaly_images)\n",
        "\n",
        "# Compute reconstruction errors for normal and anomaly images\n",
        "normal_errors  = compute_reconstruction_error(normal_images,  reconstructed_normal)\n",
        "anomaly_errors = compute_reconstruction_error(anomaly_images, reconstructed_anomaly)\n",
        "\n",
        "# Print mean reconstruction errors for normal and anomaly images\n",
        "print(\"Mean  reconstruction  error  for normal  images :\",np.mean(normal_errors))\n",
        "print(\"Mean reconstruction error for anomalous images:\", np.mean(anomaly_errors))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6q3_TEYfaox3",
        "outputId": "ce345f58-ccda-4d17-bc03-2da4670922b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "auc_roc for VAE: 0.9239\n",
            "AP for VAE: 0.787688091850614\n"
          ]
        }
      ],
      "source": [
        "# Combine the reconstruction errors and true labels\n",
        "all_errors = np.concatenate([normal_errors, anomaly_errors])\n",
        "# Define a threshold to classify anomalies based on reconstruction errors\n",
        "threshold = np.mean(all_errors)  # Adjust this threshold based on your specific use case\n",
        "\n",
        "true_labels = np.concatenate([np.zeros(len(normal_errors)), np.ones(len(anomaly_errors))])\n",
        "\n",
        "# Predict labels based on the threshold\n",
        "predicted_labels = (all_errors > threshold).astype(int)\n",
        "\n",
        "# Calculate AUC-ROC\n",
        "vae_aucroc = roc_auc_score(true_labels, all_errors)\n",
        "print(f\"auc_roc for VAE: {vae_aucroc:0.4f}\")\n",
        "\n",
        "# Calculate Average Precision (AP)\n",
        "vae_ap = average_precision_score(true_labels, all_errors)\n",
        "print(f\"AP for VAE: {average_precision}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ruUGGdbo8aEb"
      },
      "source": [
        "This is the result for the first model,not so bad, but not great either, let's go the next method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQ8PHZZPavhY"
      },
      "source": [
        "#2.Regular Autoencoder\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vLw_ItVN-8y4"
      },
      "outputs": [],
      "source": [
        "#Loading and normalizing images\n",
        "normal_images  = load_images_from_directory(normal_dir) / 255.0\n",
        "anomaly_images = load_images_from_directory(anomaly_dir) / 255.0\n",
        "mask_images    = load_images_from_directory( mask_dir ) / 255.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "YnCDGh0Xaz9s",
        "outputId": "c0b0874c-944b-46ab-c873-ae56d5823275"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_3 (InputLayer)        [(None, 128, 128, 3)]     0         \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 128, 128, 32)      896       \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPoolin  (None, 64, 64, 32)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 64, 64, 64)        18496     \n",
            "                                                                 \n",
            " max_pooling2d_4 (MaxPoolin  (None, 32, 32, 64)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 32, 32, 64)        36928     \n",
            "                                                                 \n",
            " up_sampling2d_3 (UpSamplin  (None, 64, 64, 64)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_6 (Conv2D)           (None, 64, 64, 32)        18464     \n",
            "                                                                 \n",
            " up_sampling2d_4 (UpSamplin  (None, 128, 128, 32)      0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_7 (Conv2D)           (None, 128, 128, 3)       867       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 75651 (295.51 KB)\n",
            "Trainable params: 75651 (295.51 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Define a function to build an autoencoder model for image reconstruction.\n",
        "\n",
        "def build_autoencoder(input_shape):\n",
        "    input_img = Input(shape=input_shape)  # Input layer for images of shape input_shape\n",
        "\n",
        "    # Encoder layers\n",
        "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(input_img)  # Convolutional layer with 32 filters, ReLU activation, and same padding\n",
        "    x = MaxPooling2D((2, 2), padding='same')(x)  # Max pooling layer with pool size (2, 2) and same padding\n",
        "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)  # Convolutional layer with 64 filters, ReLU activation, and same padding\n",
        "    x = MaxPooling2D((2, 2), padding='same')(x)  # Max pooling layer with pool size (2, 2) and same padding\n",
        "\n",
        "    # Decoder layers\n",
        "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)  # Convolutional layer with 64 filters, ReLU activation, and same padding\n",
        "    x = UpSampling2D((2, 2))(x)  # Up-sampling layer with up-sampling size (2, 2)\n",
        "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)  # Convolutional layer with 32 filters, ReLU activation, and same padding\n",
        "    x = UpSampling2D((2, 2))(x)  # Up-sampling layer with up-sampling size (2, 2)\n",
        "    decoded = Conv2D(3, (3, 3), activation='sigmoid', padding='same')(x)  # Convolutional layer with 3 filters, sigmoid activation, and same padding\n",
        "\n",
        "    # Define the autoencoder model\n",
        "    autoencoder = Model(input_img, decoded)  # Create a model instance mapping input_img to decoded\n",
        "    autoencoder.compile(optimizer='adam', loss='binary_crossentropy')  # Compile the model with Adam optimizer and binary cross-entropy loss\n",
        "\n",
        "    return autoencoder  # Return the constructed autoencoder model\n",
        "\n",
        "# Build the autoencoder model with input shape (128, 128, 3)\n",
        "autoencoder = build_autoencoder(input_shape=(128, 128, 3))\n",
        "\n",
        "# Print the summary of the autoencoder model architecture\n",
        "autoencoder.summary()  # Display the model layers, shapes, and number of parameters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "fSViOy-Fa0ph",
        "outputId": "bd177c7e-d94c-4cf8-e9e3-f6364676bf59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "20/20 [==============================] - 14s 624ms/step - loss: 0.5290\n",
            "Epoch 2/500\n",
            "20/20 [==============================] - 11s 545ms/step - loss: 0.4796\n",
            "Epoch 3/500\n",
            "20/20 [==============================] - 10s 483ms/step - loss: 0.4777\n",
            "Epoch 4/500\n",
            "20/20 [==============================] - 12s 589ms/step - loss: 0.4778\n",
            "Epoch 5/500\n",
            "20/20 [==============================] - 11s 579ms/step - loss: 0.4760\n",
            "Epoch 6/500\n",
            "20/20 [==============================] - 10s 466ms/step - loss: 0.4761\n",
            "Epoch 7/500\n",
            "20/20 [==============================] - 12s 590ms/step - loss: 0.4762\n",
            "Epoch 8/500\n",
            "20/20 [==============================] - 12s 583ms/step - loss: 0.4752\n",
            "Epoch 9/500\n",
            "20/20 [==============================] - 9s 465ms/step - loss: 0.4758\n",
            "Epoch 10/500\n",
            "20/20 [==============================] - 14s 697ms/step - loss: 0.4756\n",
            "Epoch 11/500\n",
            "20/20 [==============================] - 12s 588ms/step - loss: 0.4757\n",
            "Epoch 12/500\n",
            "20/20 [==============================] - 11s 541ms/step - loss: 0.4757\n",
            "Epoch 13/500\n",
            "20/20 [==============================] - 10s 492ms/step - loss: 0.4754\n",
            "Epoch 14/500\n",
            "20/20 [==============================] - 11s 577ms/step - loss: 0.4735\n",
            "Epoch 15/500\n",
            "20/20 [==============================] - 11s 572ms/step - loss: 0.4728\n",
            "Epoch 16/500\n",
            "20/20 [==============================] - 10s 462ms/step - loss: 0.4722\n",
            "Epoch 17/500\n",
            "20/20 [==============================] - 12s 589ms/step - loss: 0.4724\n",
            "Epoch 18/500\n",
            "20/20 [==============================] - 11s 574ms/step - loss: 0.4715\n",
            "Epoch 19/500\n",
            "20/20 [==============================] - 9s 457ms/step - loss: 0.4702\n",
            "Epoch 20/500\n",
            "20/20 [==============================] - 12s 594ms/step - loss: 0.4719\n",
            "Epoch 21/500\n",
            "20/20 [==============================] - 11s 576ms/step - loss: 0.4715\n",
            "Epoch 22/500\n",
            "20/20 [==============================] - 9s 464ms/step - loss: 0.4689\n",
            "Epoch 23/500\n",
            "20/20 [==============================] - 12s 594ms/step - loss: 0.4688\n",
            "Epoch 24/500\n",
            "20/20 [==============================] - 12s 588ms/step - loss: 0.4679\n",
            "Epoch 25/500\n",
            "20/20 [==============================] - 11s 533ms/step - loss: 0.4674\n",
            "Epoch 26/500\n",
            "20/20 [==============================] - 13s 634ms/step - loss: 0.4671\n",
            "Epoch 27/500\n",
            "20/20 [==============================] - 11s 566ms/step - loss: 0.4669\n",
            "Epoch 28/500\n",
            "20/20 [==============================] - 9s 463ms/step - loss: 0.4666\n",
            "Epoch 29/500\n",
            "20/20 [==============================] - 12s 587ms/step - loss: 0.4668\n",
            "Epoch 30/500\n",
            "20/20 [==============================] - 12s 583ms/step - loss: 0.4665\n",
            "Epoch 31/500\n",
            "20/20 [==============================] - 9s 459ms/step - loss: 0.4666\n",
            "Epoch 32/500\n",
            "20/20 [==============================] - 12s 589ms/step - loss: 0.4669\n",
            "Epoch 33/500\n",
            "20/20 [==============================] - 12s 583ms/step - loss: 0.4667\n",
            "Epoch 34/500\n",
            "20/20 [==============================] - 9s 473ms/step - loss: 0.4665\n",
            "Epoch 35/500\n",
            "20/20 [==============================] - 12s 563ms/step - loss: 0.4664\n",
            "Epoch 36/500\n",
            "20/20 [==============================] - 12s 582ms/step - loss: 0.4664\n",
            "Epoch 37/500\n",
            "20/20 [==============================] - 9s 474ms/step - loss: 0.4664\n",
            "Epoch 38/500\n",
            "20/20 [==============================] - 11s 557ms/step - loss: 0.4664\n",
            "Epoch 39/500\n",
            "20/20 [==============================] - 12s 597ms/step - loss: 0.4664\n",
            "Epoch 40/500\n",
            "20/20 [==============================] - 10s 484ms/step - loss: 0.4666\n",
            "Epoch 41/500\n",
            "20/20 [==============================] - 13s 657ms/step - loss: 0.4665\n",
            "Epoch 42/500\n",
            "20/20 [==============================] - 11s 580ms/step - loss: 0.4666\n",
            "Epoch 43/500\n",
            "20/20 [==============================] - 9s 453ms/step - loss: 0.4669\n",
            "Epoch 44/500\n",
            "20/20 [==============================] - 11s 578ms/step - loss: 0.4672\n",
            "Epoch 45/500\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.4666Restoring model weights from the end of the best epoch: 39.\n",
            "20/20 [==============================] - 12s 582ms/step - loss: 0.4666\n",
            "Epoch 45: early stopping\n"
          ]
        }
      ],
      "source": [
        "# Fit the autoencoder model on normal_images for image reconstruction.\n",
        "\n",
        "history = autoencoder.fit(normal_images, normal_images,  # Train the model on normal_images for reconstruction\n",
        "                          epochs=500,  # Number of training epochs\n",
        "                          batch_size=8,  # Batch size for each iteration\n",
        "                          verbose=1,  # Verbosity mode (1: progress bar, 0: silent)\n",
        "                          callbacks=EarlyStopping(  # Early stopping callback to prevent overfitting\n",
        "                              monitor=\"loss\",  # Monitor training loss\n",
        "                              verbose=1,  # Verbosity mode (1: progress bar, 0: silent)\n",
        "                              patience=6,  # Number of epochs with no improvement after which training will be stopped\n",
        "                              mode='min',  # Direction of improvement (minimize training loss)\n",
        "                              restore_best_weights=True  # Restore the model weights from the epoch with the best training loss\n",
        "                          )\n",
        "                         )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "collapsed": true,
        "id": "eIlfDFvnhJby",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3167b592-c16a-4df6-8f07-24ed3f5f2c1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2/2 [==============================] - 1s 208ms/step\n"
          ]
        }
      ],
      "source": [
        "# Predict on anomaly images\n",
        "reconstructed_anomalies = autoencoder.predict(anomaly_images)\n",
        "\n",
        "# Calculate reconstruction error\n",
        "reconstruction_errors = compute_reconstruction_error(anomaly_images, reconstructed_anomalies)\n",
        "\n",
        "# Threshold for anomaly detection (this can be tuned)\n",
        "threshold = np.percentile(reconstruction_errors, 95)\n",
        "\n",
        "# Detect anomalies\n",
        "anomalies_detected = reconstruction_errors > threshold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "collapsed": true,
        "id": "fRT5ToRDjFA1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2acef1c8-d381-41db-8798-1f269388de00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5/5 [==============================] - 3s 521ms/step\n",
            "2/2 [==============================] - 1s 129ms/step\n",
            "AUROC for autoencoder: 0.9838\n",
            "AP for autoencoder: 0.9578\n"
          ]
        }
      ],
      "source": [
        "# Predict on both good and anomaly images\n",
        "reconstructed_good = autoencoder.predict(normal_images)\n",
        "reconstructed_anomalies = autoencoder.predict(anomaly_images)\n",
        "\n",
        "# Calculate reconstruction error for both\n",
        "good_errors = compute_reconstruction_error(normal_images, reconstructed_good)\n",
        "anomaly_errors = compute_reconstruction_error(anomaly_images, reconstructed_anomalies)\n",
        "\n",
        "# Combine errors and labels\n",
        "all_errors = np.concatenate([good_errors, anomaly_errors])\n",
        "all_labels = np.concatenate([np.zeros(len(good_errors)), np.ones(len(anomaly_errors))])\n",
        "\n",
        "# Compute AUROC\n",
        "ae_auroc = roc_auc_score(all_labels, all_errors)\n",
        "print(f'AUROC for autoencoder: {ae_auroc:.4f}')\n",
        "\n",
        "# Compute Average Precision (AP)\n",
        "au_ap = average_precision_score(all_labels, all_errors)\n",
        "print(f'AP for autoencoder: {au_ap:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jk26mZENyIZx"
      },
      "source": [
        "#One-class SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VCh14J4GyNDZ",
        "outputId": "302aa393-5f4b-47d6-d7e2-8363c820bef9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AUC-ROC for ocsvm: 0.9522292993630573\n",
            "AP for ocsvm: 0.9806007307058101\n"
          ]
        }
      ],
      "source": [
        "# Flatten the images\n",
        "X_flat = normal_images.reshape(normal_images.shape[0], -1)\n",
        "# Scale the data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X_flat)\n",
        "# Train One-Class SVM\n",
        "ocsvm = OneClassSVM(kernel='rbf', nu=0.1)  # Adjust parameters as needed\n",
        "ocsvm.fit(X_scaled)\n",
        "\n",
        "anomaly_scaled = anomaly_images.reshape(anomaly_images.shape[0], -1)\n",
        "anomaly_scaled = scaler.transform(anomaly_scaled)\n",
        "\n",
        "\n",
        "# Predict images\n",
        "all_images = np.concatenate((X_scaled, anomaly_scaled), axis=0)\n",
        "all_labels = np.concatenate((np.ones(X_scaled.shape[0]) , -1*np.ones(anomaly_scaled.shape[0])))\n",
        "\n",
        "all_prediction = ocsvm.predict(all_images)\n",
        "\n",
        "ocsvm_aucroc = roc_auc_score(all_labels, all_prediction)\n",
        "print(f\"AUC-ROC for ocsvm: {ocsvm_aucroc}\")\n",
        "\n",
        "# Calculate Average Precision (AP)\n",
        "ocsvm_ap = average_precision_score(all_labels, all_prediction)\n",
        "print(f\"AP for ocsvm: {ocsvm_ap}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlnFlW0Szi95"
      },
      "source": [
        "#Isonation Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZgBFenzzU0d"
      },
      "outputs": [],
      "source": [
        "X_good = flatten_images(normal_images)\n",
        "X_anomaly = flatten_images(anomaly_images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "abhMCExV0UhD"
      },
      "outputs": [],
      "source": [
        "# Scale the data\n",
        "scaler = StandardScaler()\n",
        "X_good = scaler.fit_transform(X_good)\n",
        "X_anomaly = scaler.transform(X_anomaly)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "orQnHUUx0aZy"
      },
      "outputs": [],
      "source": [
        "# Define the parameter grid\n",
        "# param_grid = {\n",
        "#     'n_estimators': [100, 200, 300],\n",
        "#     'max_samples': [128, 256, 512, 'auto'],\n",
        "#     'contamination': [0.01, 0.05, 0.1],\n",
        "#     'max_features': [0.5, 0.75, 1.0],\n",
        "#     'bootstrap': [True, False]\n",
        "# }\n",
        "\n",
        "param_grid = {\n",
        "    'n_estimators': [100],\n",
        "    'max_samples': ['auto'],\n",
        "    'contamination': [0.01, 0.05, 0.1],\n",
        "    'max_features': [0.5, 0.75, 1.0],\n",
        "    'bootstrap': [False]\n",
        "}\n",
        "\n",
        "# Initialize the Isolation Forest model\n",
        "isolation_forest = IsolationForest(random_state=42)\n",
        "\n",
        "# Initialize the GridSearchCV object\n",
        "grid_search = GridSearchCV(estimator=isolation_forest, param_grid=param_grid, scoring='roc_auc', cv=5, n_jobs=-1)\n",
        "\n",
        "grid_search.fit(X_good)\n",
        "\n",
        "# Print the best parameters and the corresponding score\n",
        "print(\"Best parameters found: \", grid_search.best_params_)\n",
        "print(\"Best AUCROC score: \", grid_search.best_score_)\n",
        "\n",
        "\n",
        "all_images = np.concatenate((X_good, X_anomaly), axis=0)\n",
        "all_labels = np.concatenate((np.ones(X_good.shape[0]) , -1*np.ones(X_anomaly.shape[0])))\n",
        "\n",
        "predictions = isolation_forest.predict(all_images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wh8E2H-x0hDs"
      },
      "outputs": [],
      "source": [
        "# Compute AUCROC\n",
        "aucroc = roc_auc_score(predictions, all_labels)\n",
        "\n",
        "# Compute AP\n",
        "ap = average_precision_score(predictions, all_labels)\n",
        "\n",
        "print(f'AUCROC: {aucroc}')\n",
        "print(f'AP: {ap}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dr67eG2J0maF"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyPx9hdGFcmVNtXOGarDfNZM",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}