{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rezamohammadloo/Surface_anomaly_detection_using_unsupervised_algorithms/blob/main/_Surface_Defect_Detection_Unsupervised.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-zig6s0uMPe"
      },
      "source": [
        "Hi, this is a project that has been done to implement some unsupervised algorhimgs for \"Anomaly Detection\" or \"Surface Anomaly Detection\" problem on an image dataset with image sizes: 128*128*3.\n",
        "4 different methodes are implemented for this purpose and the result of them are evaluated by AUROC and AP techniques to identify the best one.\n",
        "The first algorithm is a VAE (Varitional Autoencoer)\n",
        "the second one is a regular autoencoder.\n",
        "the third one is OCSVM(One-Class Support Vector Machine)\n",
        "and finally the last algorithm is Isolation Forest.\n",
        "Befor designing and trainin model some of the main libararies and packages have been implemented to use and some functions and variables that are in common between methodes are defined.\n",
        "Let's dive in!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pHA9PY7DWYb5",
        "outputId": "b29806c1-6235-4af2-f15b-f95c744726f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Mounting Google Drive to access the dataset stored in my Google Drive account\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "tixxm-fAYGDh"
      },
      "outputs": [],
      "source": [
        "# Importing necessary libraries for building and training models\n",
        "import os  # For operating system dependent functionality\n",
        "import numpy as np  # For numerical operations and array manipulations\n",
        "import tensorflow as tf  # For deep learning model creation and training\n",
        "import matplotlib.pyplot as plt  # For plotting and visualizing data\n",
        "from sklearn.svm import OneClassSVM  # For One-Class SVM, a traditional anomaly detection method\n",
        "from sklearn.pipeline import Pipeline  # For creating pipelines that streamline preprocessing and model training\n",
        "from tensorflow.keras.models import Model  # For building Keras models\n",
        "from sklearn.ensemble import IsolationForest  # For Isolation Forest, another traditional anomaly detection method\n",
        "from sklearn.preprocessing import StandardScaler  # For standardizing features by removing the mean and scaling to unit variance\n",
        "from sklearn.model_selection import GridSearchCV  # For performing hyperparameter tuning using grid search\n",
        "from tensorflow.keras.callbacks import EarlyStopping  # For stopping training early if the model stops improving\n",
        "from sklearn.model_selection import train_test_split  # For splitting data into training and testing sets\n",
        "from tensorflow.keras import layers, models, backend as K  # For creating layers, models, and accessing Keras backend\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score  # For evaluating model performance using ROC AUC and average precision\n",
        "from tensorflow.keras.preprocessing.image import img_to_array, load_img  # For converting images to arrays and loading images\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D  # For creating convolutional autoencoders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ogI-pnC0m2oS"
      },
      "source": [
        "#0.General Functions and parameters\n",
        "There are some finctions and varaiables that will be used in some or all of the methods.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "loHE_e8srCst"
      },
      "outputs": [],
      "source": [
        "# Define a function to load images from a specified directories\n",
        "def load_images_from_directory(directory, target_size=(128, 128)):\n",
        "    images = []\n",
        "    for filename in os.listdir(directory):\n",
        "        if filename.endswith('.png') or filename.endswith('.jpg'):\n",
        "            img = load_img(os.path.join(directory, filename), target_size=target_size)\n",
        "            img_array = img_to_array(img) #/ 255.0  #Deviding by 255 in canceled here because we don't need to do it for third and furth method and we'll do it in the first and second method by hand.\n",
        "            images.append(img_array)\n",
        "    return np.array(images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "JuI59Jwrtfdc"
      },
      "outputs": [],
      "source": [
        "# Directories\n",
        "# Specify the directories containing normal, anomaly, and mask images\n",
        "normal_dir  = '/content/gdrive/MyDrive/glass-defect-sample/good'\n",
        "anomaly_dir = '/content/gdrive/MyDrive/glass-defect-sample/anomaly'\n",
        "mask_dir    = '/content/gdrive/MyDrive/glass-defect-sample/mask'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "mE1_M4Hkr1e8"
      },
      "outputs": [],
      "source": [
        "# Flatten images to vectors, it will be used for third and furth methodes\n",
        "def flatten_images(images):\n",
        "    return images.reshape(images.shape[0], -1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "zFcJ-Tjl0gDg"
      },
      "outputs": [],
      "source": [
        "# Define a function to compute reconstruction errors.\n",
        "def compute_reconstruction_error(original, reconstructed):\n",
        "    return np.mean(np.abs(original - reconstructed), axis=(1, 2, 3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ufu9Hy2iYGBa",
        "outputId": "13b371fb-c3a8-4a95-9390-980fe2383ed0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Normal Images Shape: (157, 128, 128, 3)\n",
            "Anomaly Images Shape: (40, 128, 128, 3)\n",
            "Mask Images Shape: (40, 128, 128, 3)\n"
          ]
        }
      ],
      "source": [
        "# Load data\n",
        "# Load the normal, anomaly, and mask images from their respective directories\n",
        "normal_images  = load_images_from_directory(normal_dir)\n",
        "anomaly_images = load_images_from_directory(anomaly_dir)\n",
        "mask_images    = load_images_from_directory( mask_dir )\n",
        "\n",
        "# Print the shape of the loaded images\n",
        "print(\"Normal Images Shape:\", normal_images.shape)\n",
        "print(\"Anomaly Images Shape:\", anomaly_images.shape)\n",
        "print(\"Mask Images Shape:\", mask_images.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NdayKebpY88a"
      },
      "source": [
        "#1.Varational Autoencoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CojbIrUcyWqc"
      },
      "source": [
        "First we normalize loaded images to make the most use out of them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "UsHT9Ewktqxe"
      },
      "outputs": [],
      "source": [
        "#Normalizing their values to [0, 1]\n",
        "normal_images  =  normal_images / 255.0\n",
        "anomaly_images =  anomaly_images / 255.0\n",
        "mask_images    =  mask_images / 255.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b94RfoFF8pVj"
      },
      "source": [
        "Now it's time to design the architecture of VAE."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "AGkSYlXFZOdZ"
      },
      "outputs": [],
      "source": [
        "latent_dim = 32 #Dimensionality of the latent space for the Variational Autoencoder (VAE)\n",
        "\n",
        "# Encoder model definition\n",
        "def encoder_model(input_shape):\n",
        "    # Define input layer\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "\n",
        "    # Convolutional layers with ReLU activation and same padding\n",
        "    x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n",
        "    x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
        "\n",
        "    x = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
        "\n",
        "    x = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
        "\n",
        "    # Flatten the output and pass through dense layers to get z_mean and z_log_var\n",
        "    x = layers.Flatten()(x)\n",
        "    x = layers.Dense(128, activation='relu')(x)\n",
        "    z_mean = layers.Dense(latent_dim)(x)      # Mean of the latent space\n",
        "    z_log_var = layers.Dense(latent_dim)(x)   # Log variance of the latent space\n",
        "\n",
        "    # Define and return the encoder model\n",
        "    return models.Model(inputs, [z_mean, z_log_var], name='encoder')\n",
        "\n",
        "# Sampling function definition for the VAE\n",
        "def sampling(args):\n",
        "    # Retrieve z_mean and z_log_var from arguments\n",
        "    z_mean, z_log_var = args\n",
        "\n",
        "    # Determine batch size and dimensionality of z_mean\n",
        "    batch = K.shape(z_mean)[0]\n",
        "    dim = K.int_shape(z_mean)[1]\n",
        "\n",
        "    # Sample epsilon from normal distribution\n",
        "    epsilon = K.random_normal(shape=(batch, dim))\n",
        "\n",
        "    # Reparameterization trick: sample from the learned distribution\n",
        "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
        "\n",
        "# Decoder model definition\n",
        "def decoder_model(latent_dim, input_shape):\n",
        "    # Define input layer for latent space\n",
        "    latent_inputs = layers.Input(shape=(latent_dim,))\n",
        "\n",
        "    # Dense layer to reshape and feed into convolutional layers\n",
        "    x = layers.Dense((input_shape[0] // 8) * (input_shape[1] // 8) * 128, activation='relu')(latent_inputs)\n",
        "    x = layers.Reshape((input_shape[0] // 8, input_shape[1] // 8, 128))(x)\n",
        "\n",
        "    # Convolutional transpose layers for upsampling and decoding\n",
        "    x = layers.Conv2DTranspose(128, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = layers.UpSampling2D((2, 2))(x)\n",
        "\n",
        "    x = layers.Conv2DTranspose(64, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = layers.UpSampling2D((2, 2))(x)\n",
        "\n",
        "    x = layers.Conv2DTranspose(32, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = layers.UpSampling2D((2, 2))(x)\n",
        "\n",
        "    # Output layer with sigmoid activation for image reconstruction\n",
        "    outputs = layers.Conv2DTranspose(3, (3, 3), activation='sigmoid', padding='same')(x)\n",
        "\n",
        "    # Define and return the decoder model\n",
        "    return models.Model(latent_inputs, outputs, name='decoder')\n",
        "\n",
        "# Define input shape for the VAE\n",
        "input_shape = (128, 128, 3)\n",
        "\n",
        "# Build the encoder model\n",
        "encoder = encoder_model(input_shape)\n",
        "z_mean, z_log_var = encoder.output\n",
        "\n",
        "# Use the sampling function to obtain latent space representation\n",
        "z = layers.Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n",
        "\n",
        "# Build the decoder model using the latent space representation\n",
        "decoder = decoder_model(latent_dim, input_shape)\n",
        "\n",
        "# Connect encoder input to decoder output to create VAE model\n",
        "outputs = decoder(z)\n",
        "\n",
        "# Define the VAE model with encoder input and decoder output\n",
        "vae = models.Model(encoder.input, outputs, name='vae')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zS1t100188hH"
      },
      "source": [
        "For traing this model we need to combine two different loss funcions: \"binary cross entropy\" and \"KL-divergence\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "L3E483f0ZW8k"
      },
      "outputs": [],
      "source": [
        "# Calculate the reconstruction loss using binary cross-entropy\n",
        "reconstruction_loss = tf.keras.losses.binary_crossentropy(K.flatten(encoder.input), K.flatten(outputs))\n",
        "# Scale the reconstruction loss based on the input shape\n",
        "reconstruction_loss *= input_shape[0] * input_shape[1] * input_shape[2]\n",
        "\n",
        "# Calculate the KL divergence loss\n",
        "kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n",
        "kl_loss = K.sum(kl_loss, axis=-1)\n",
        "kl_loss *= -0.5\n",
        "\n",
        "# Calculate the total VAE loss by combining the reconstruction and KL divergence losses\n",
        "vae_loss = K.mean(reconstruction_loss + kl_loss)\n",
        "\n",
        "# Add the VAE loss to the model\n",
        "vae.add_loss(vae_loss)\n",
        "\n",
        "# Compile the VAE model with the Adam optimizer\n",
        "vae.compile(optimizer='adam')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "1S_XdkowZevD",
        "outputId": "24f94336-8a06-40a3-d448-25ce63b7f12c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "20/20 [==============================] - 19s 651ms/step - loss: 26085.1445\n",
            "Epoch 2/500\n",
            "20/20 [==============================] - 14s 707ms/step - loss: 23985.5586\n",
            "Epoch 3/500\n",
            "20/20 [==============================] - 14s 716ms/step - loss: 24048.7891\n",
            "Epoch 4/500\n",
            "20/20 [==============================] - 14s 683ms/step - loss: 23985.5566\n",
            "Epoch 5/500\n",
            "20/20 [==============================] - 12s 623ms/step - loss: 23622.1035\n",
            "Epoch 6/500\n",
            "20/20 [==============================] - 12s 597ms/step - loss: 23625.8945\n",
            "Epoch 7/500\n",
            "20/20 [==============================] - 14s 694ms/step - loss: 23751.9531\n",
            "Epoch 8/500\n",
            "20/20 [==============================] - 14s 715ms/step - loss: 23526.6582\n",
            "Epoch 9/500\n",
            "20/20 [==============================] - 14s 713ms/step - loss: 23467.2109\n",
            "Epoch 10/500\n",
            "20/20 [==============================] - 14s 719ms/step - loss: 23455.9727\n",
            "Epoch 11/500\n",
            "20/20 [==============================] - 12s 606ms/step - loss: 23374.2988\n",
            "Epoch 12/500\n",
            "20/20 [==============================] - 14s 681ms/step - loss: 23391.7285\n",
            "Epoch 13/500\n",
            "20/20 [==============================] - 16s 806ms/step - loss: 23417.2148\n",
            "Epoch 14/500\n",
            "20/20 [==============================] - 14s 711ms/step - loss: 23353.7324\n",
            "Epoch 15/500\n",
            "20/20 [==============================] - 14s 704ms/step - loss: 23192.3320\n",
            "Epoch 16/500\n",
            "20/20 [==============================] - 13s 624ms/step - loss: 23213.5293\n",
            "Epoch 17/500\n",
            "20/20 [==============================] - 12s 605ms/step - loss: 23154.4277\n",
            "Epoch 18/500\n",
            "20/20 [==============================] - 14s 701ms/step - loss: 23190.8086\n",
            "Epoch 19/500\n",
            "20/20 [==============================] - 14s 696ms/step - loss: 23185.4219\n",
            "Epoch 20/500\n",
            "20/20 [==============================] - 14s 706ms/step - loss: 23420.1992\n",
            "Epoch 21/500\n",
            "20/20 [==============================] - 13s 667ms/step - loss: 23228.2637\n",
            "Epoch 22/500\n",
            "20/20 [==============================] - 12s 581ms/step - loss: 23151.7617\n",
            "Epoch 23/500\n",
            "20/20 [==============================] - 17s 852ms/step - loss: 23226.2559\n",
            "Epoch 24/500\n",
            "20/20 [==============================] - 14s 698ms/step - loss: 23193.9883\n",
            "Epoch 25/500\n",
            "20/20 [==============================] - 15s 768ms/step - loss: 23064.7324\n",
            "Epoch 26/500\n",
            "20/20 [==============================] - 14s 694ms/step - loss: 23255.2148\n",
            "Epoch 27/500\n",
            "20/20 [==============================] - 14s 684ms/step - loss: 23142.9551\n",
            "Epoch 28/500\n",
            "20/20 [==============================] - 12s 582ms/step - loss: 22992.9648\n",
            "Epoch 29/500\n",
            "20/20 [==============================] - 12s 606ms/step - loss: 23029.0879\n",
            "Epoch 30/500\n",
            "20/20 [==============================] - 14s 691ms/step - loss: 23084.4590\n",
            "Epoch 31/500\n",
            "20/20 [==============================] - 14s 692ms/step - loss: 23014.7168\n",
            "Epoch 32/500\n",
            "20/20 [==============================] - 13s 671ms/step - loss: 23002.8418\n",
            "Epoch 33/500\n",
            "20/20 [==============================] - 11s 544ms/step - loss: 23026.4023\n",
            "Epoch 34/500\n",
            "20/20 [==============================] - ETA: 0s - loss: 23009.9062Restoring model weights from the end of the best epoch: 28.\n",
            "20/20 [==============================] - 13s 661ms/step - loss: 23009.9062\n",
            "Epoch 34: early stopping\n"
          ]
        }
      ],
      "source": [
        "# Define the history object to store training metrics\n",
        "history = vae.fit(\n",
        "    # Train the VAE model on normal images with normal images as both input and target\n",
        "    normal_images, normal_images,\n",
        "    epochs=500,  # Number of epochs for training\n",
        "    batch_size=8,  # Batch size for training\n",
        "    #validation_split=0.2,  # Fraction of training data to use for validation\n",
        "    callbacks=EarlyStopping(  # Early stopping callback to prevent overfitting\n",
        "        monitor=\"loss\",  # Monitor validation loss\n",
        "        verbose=1,  # Verbosity mode (1: progress bar, 0: silent)\n",
        "        patience=6,  # Number of epochs with no improvement after which training will be stopped\n",
        "        mode='min',  # Direction of improvement (minimize validation loss)\n",
        "        restore_best_weights=True  # Restore the model weights from the epoch with the best validation loss\n",
        "    )\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWiOOuQs99Z9"
      },
      "source": [
        "Reconstructing images using designed model in order to determice threshold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "_ZR7pb_kaNhw",
        "outputId": "a88d3c1a-b79a-4c3a-a3aa-016cf6d5b4fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5/5 [==============================] - 4s 678ms/step\n",
            "2/2 [==============================] - 1s 159ms/step\n",
            "Mean  reconstruction  error  for normal  images : 0.021452293\n",
            "Mean reconstruction error for anomalous images: 0.09324009\n"
          ]
        }
      ],
      "source": [
        "# Reconstruct normal and anomaly images using the trained VAE model\n",
        "reconstructed_normal  = vae.predict(normal_images)\n",
        "reconstructed_anomaly = vae.predict(anomaly_images)\n",
        "\n",
        "# Compute reconstruction errors for normal and anomaly images\n",
        "normal_errors  = compute_reconstruction_error(normal_images,  reconstructed_normal)\n",
        "anomaly_errors = compute_reconstruction_error(anomaly_images, reconstructed_anomaly)\n",
        "\n",
        "# Print mean reconstruction errors for normal and anomaly images\n",
        "print(\"Mean  reconstruction  error  for normal  images :\",np.mean(normal_errors))\n",
        "print(\"Mean reconstruction error for anomalous images:\", np.mean(anomaly_errors))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6q3_TEYfaox3",
        "outputId": "ce345f58-ccda-4d17-bc03-2da4670922b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "auc_roc for VAE: 0.9239\n",
            "AP for VAE: 0.787688091850614\n"
          ]
        }
      ],
      "source": [
        "# Combine the reconstruction errors and true labels\n",
        "all_errors = np.concatenate([normal_errors, anomaly_errors])\n",
        "# Define a threshold to classify anomalies based on reconstruction errors\n",
        "threshold = np.mean(all_errors)  # Adjust this threshold based on your specific use case\n",
        "\n",
        "true_labels = np.concatenate([np.zeros(len(normal_errors)), np.ones(len(anomaly_errors))])\n",
        "\n",
        "# Predict labels based on the threshold\n",
        "predicted_labels = (all_errors > threshold).astype(int)\n",
        "\n",
        "# Calculate AUC-ROC\n",
        "vae_aucroc = roc_auc_score(true_labels, all_errors)\n",
        "print(f\"auc_roc for VAE: {vae_aucroc:0.4f}\")\n",
        "\n",
        "# Calculate Average Precision (AP)\n",
        "vae_ap = average_precision_score(true_labels, all_errors)\n",
        "print(f\"AP for VAE: {average_precision}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ruUGGdbo8aEb"
      },
      "source": [
        "This is the result for the first model,not so bad, but not great either, let's go the next method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQ8PHZZPavhY"
      },
      "source": [
        "#2.Regular Autoencoder\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vLw_ItVN-8y4"
      },
      "outputs": [],
      "source": [
        "#Loading and normalizing images\n",
        "normal_images  = load_images_from_directory(normal_dir) / 255.0\n",
        "anomaly_images = load_images_from_directory(anomaly_dir) / 255.0\n",
        "mask_images    = load_images_from_directory( mask_dir ) / 255.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "YnCDGh0Xaz9s",
        "outputId": "85a1a895-d185-4bd0-a493-6d9480583475"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_10 (InputLayer)       [(None, 128, 128, 3)]     0         \n",
            "                                                                 \n",
            " conv2d_15 (Conv2D)          (None, 128, 128, 32)      896       \n",
            "                                                                 \n",
            " max_pooling2d_15 (MaxPooli  (None, 64, 64, 32)        0         \n",
            " ng2D)                                                           \n",
            "                                                                 \n",
            " conv2d_16 (Conv2D)          (None, 64, 64, 64)        18496     \n",
            "                                                                 \n",
            " max_pooling2d_16 (MaxPooli  (None, 32, 32, 64)        0         \n",
            " ng2D)                                                           \n",
            "                                                                 \n",
            " conv2d_17 (Conv2D)          (None, 32, 32, 64)        36928     \n",
            "                                                                 \n",
            " up_sampling2d_12 (UpSampli  (None, 64, 64, 64)        0         \n",
            " ng2D)                                                           \n",
            "                                                                 \n",
            " conv2d_18 (Conv2D)          (None, 64, 64, 32)        18464     \n",
            "                                                                 \n",
            " up_sampling2d_13 (UpSampli  (None, 128, 128, 32)      0         \n",
            " ng2D)                                                           \n",
            "                                                                 \n",
            " conv2d_19 (Conv2D)          (None, 128, 128, 3)       867       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 75651 (295.51 KB)\n",
            "Trainable params: 75651 (295.51 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Define a function to build an autoencoder model for image reconstruction.\n",
        "\n",
        "def build_autoencoder(input_shape):\n",
        "    input_img = Input(shape=input_shape)  # Input layer for images of shape input_shape\n",
        "\n",
        "    # Encoder layers\n",
        "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(input_img)  # Convolutional layer with 32 filters, ReLU activation, and same padding\n",
        "    x = MaxPooling2D((2, 2), padding='same')(x)  # Max pooling layer with pool size (2, 2) and same padding\n",
        "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)  # Convolutional layer with 64 filters, ReLU activation, and same padding\n",
        "    x = MaxPooling2D((2, 2), padding='same')(x)  # Max pooling layer with pool size (2, 2) and same padding\n",
        "\n",
        "    # Decoder layers\n",
        "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)  # Convolutional layer with 64 filters, ReLU activation, and same padding\n",
        "    x = UpSampling2D((2, 2))(x)  # Up-sampling layer with up-sampling size (2, 2)\n",
        "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)  # Convolutional layer with 32 filters, ReLU activation, and same padding\n",
        "    x = UpSampling2D((2, 2))(x)  # Up-sampling layer with up-sampling size (2, 2)\n",
        "    decoded = Conv2D(3, (3, 3), activation='sigmoid', padding='same')(x)  # Convolutional layer with 3 filters, sigmoid activation, and same padding\n",
        "\n",
        "    # Define the autoencoder model\n",
        "    autoencoder = Model(input_img, decoded)  # Create a model instance mapping input_img to decoded\n",
        "    autoencoder.compile(optimizer='adam', loss='binary_crossentropy')  # Compile the model with Adam optimizer and binary cross-entropy loss\n",
        "\n",
        "    return autoencoder  # Return the constructed autoencoder model\n",
        "\n",
        "# Build the autoencoder model with input shape (128, 128, 3)\n",
        "autoencoder = build_autoencoder(input_shape=(128, 128, 3))\n",
        "\n",
        "# Print the summary of the autoencoder model architecture\n",
        "autoencoder.summary()  # Display the model layers, shapes, and number of parameters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "fSViOy-Fa0ph",
        "outputId": "5571499c-ffbd-4a1c-9875-3c1e211e4a7c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "20/20 [==============================] - 15s 629ms/step - loss: 0.5156\n",
            "Epoch 2/500\n",
            "20/20 [==============================] - 9s 452ms/step - loss: 0.4789\n",
            "Epoch 3/500\n",
            "20/20 [==============================] - 11s 580ms/step - loss: 0.4768\n",
            "Epoch 4/500\n",
            "20/20 [==============================] - 14s 694ms/step - loss: 0.4761\n",
            "Epoch 5/500\n",
            "20/20 [==============================] - 9s 435ms/step - loss: 0.4756\n",
            "Epoch 6/500\n",
            "20/20 [==============================] - 20s 1s/step - loss: 0.4749\n",
            "Epoch 7/500\n",
            "20/20 [==============================] - 18s 862ms/step - loss: 0.4753\n",
            "Epoch 8/500\n",
            "20/20 [==============================] - 12s 548ms/step - loss: 0.4749\n",
            "Epoch 9/500\n",
            "20/20 [==============================] - 12s 592ms/step - loss: 0.4751\n",
            "Epoch 10/500\n",
            "20/20 [==============================] - 10s 526ms/step - loss: 0.4739\n",
            "Epoch 11/500\n",
            "20/20 [==============================] - 10s 464ms/step - loss: 0.4736\n",
            "Epoch 12/500\n",
            "20/20 [==============================] - 11s 558ms/step - loss: 0.4731\n",
            "Epoch 13/500\n",
            "20/20 [==============================] - 10s 480ms/step - loss: 0.4730\n",
            "Epoch 14/500\n",
            "20/20 [==============================] - 10s 501ms/step - loss: 0.4741\n",
            "Epoch 15/500\n",
            "20/20 [==============================] - 11s 572ms/step - loss: 0.4768\n",
            "Epoch 16/500\n",
            "20/20 [==============================] - 9s 429ms/step - loss: 0.4738\n",
            "Epoch 17/500\n",
            "20/20 [==============================] - 12s 587ms/step - loss: 0.4723\n",
            "Epoch 18/500\n",
            "20/20 [==============================] - 13s 675ms/step - loss: 0.4719\n",
            "Epoch 19/500\n",
            "20/20 [==============================] - 10s 510ms/step - loss: 0.4717\n",
            "Epoch 20/500\n",
            "20/20 [==============================] - 10s 496ms/step - loss: 0.4704\n",
            "Epoch 21/500\n",
            "20/20 [==============================] - 11s 566ms/step - loss: 0.4682\n",
            "Epoch 22/500\n",
            "20/20 [==============================] - 8s 424ms/step - loss: 0.4671\n",
            "Epoch 23/500\n",
            "20/20 [==============================] - 11s 541ms/step - loss: 0.4668\n",
            "Epoch 24/500\n",
            "20/20 [==============================] - 11s 542ms/step - loss: 0.4666\n",
            "Epoch 25/500\n",
            "20/20 [==============================] - 8s 421ms/step - loss: 0.4667\n",
            "Epoch 26/500\n",
            "20/20 [==============================] - 11s 549ms/step - loss: 0.4670\n",
            "Epoch 27/500\n",
            "20/20 [==============================] - 9s 473ms/step - loss: 0.4666\n",
            "Epoch 28/500\n",
            "20/20 [==============================] - 10s 479ms/step - loss: 0.4665\n",
            "Epoch 29/500\n",
            "20/20 [==============================] - 11s 537ms/step - loss: 0.4664\n",
            "Epoch 30/500\n",
            "20/20 [==============================] - 8s 422ms/step - loss: 0.4664\n",
            "Epoch 31/500\n",
            "20/20 [==============================] - 11s 541ms/step - loss: 0.4664\n",
            "Epoch 32/500\n",
            "20/20 [==============================] - 11s 551ms/step - loss: 0.4664\n",
            "Epoch 33/500\n",
            "20/20 [==============================] - 10s 462ms/step - loss: 0.4664\n",
            "Epoch 34/500\n",
            "20/20 [==============================] - 13s 641ms/step - loss: 0.4664\n",
            "Epoch 35/500\n",
            "20/20 [==============================] - 10s 487ms/step - loss: 0.4664\n",
            "Epoch 36/500\n",
            "20/20 [==============================] - 10s 497ms/step - loss: 0.4663\n",
            "Epoch 37/500\n",
            "20/20 [==============================] - 11s 568ms/step - loss: 0.4663\n",
            "Epoch 38/500\n",
            "20/20 [==============================] - 9s 431ms/step - loss: 0.4664\n",
            "Epoch 39/500\n",
            "20/20 [==============================] - 11s 552ms/step - loss: 0.4665\n",
            "Epoch 40/500\n",
            "20/20 [==============================] - 11s 536ms/step - loss: 0.4664\n",
            "Epoch 41/500\n",
            "20/20 [==============================] - 9s 450ms/step - loss: 0.4663\n",
            "Epoch 42/500\n",
            "20/20 [==============================] - 11s 564ms/step - loss: 0.4663\n",
            "Epoch 43/500\n",
            "20/20 [==============================] - 10s 491ms/step - loss: 0.4663\n",
            "Epoch 44/500\n",
            "20/20 [==============================] - 10s 491ms/step - loss: 0.4663\n",
            "Epoch 45/500\n",
            "20/20 [==============================] - 11s 574ms/step - loss: 0.4663\n",
            "Epoch 46/500\n",
            "20/20 [==============================] - 9s 452ms/step - loss: 0.4663\n",
            "Epoch 47/500\n",
            "20/20 [==============================] - 11s 558ms/step - loss: 0.4665\n",
            "Epoch 48/500\n",
            "20/20 [==============================] - 11s 578ms/step - loss: 0.4666\n",
            "Epoch 49/500\n",
            "20/20 [==============================] - 12s 618ms/step - loss: 0.4667\n",
            "Epoch 50/500\n",
            "20/20 [==============================] - 11s 531ms/step - loss: 0.4666\n",
            "Epoch 51/500\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.4666Restoring model weights from the end of the best epoch: 45.\n",
            "20/20 [==============================] - 11s 564ms/step - loss: 0.4666\n",
            "Epoch 51: early stopping\n"
          ]
        }
      ],
      "source": [
        "# Fit the autoencoder model on normal_images for image reconstruction.\n",
        "\n",
        "history = autoencoder.fit(normal_images, normal_images,  # Train the model on normal_images for reconstruction\n",
        "                          epochs=500,  # Number of training epochs\n",
        "                          batch_size=8,  # Batch size for each iteration\n",
        "                          verbose=1,  # Verbosity mode (1: progress bar, 0: silent)\n",
        "                          callbacks=EarlyStopping(  # Early stopping callback to prevent overfitting\n",
        "                              monitor=\"loss\",  # Monitor training loss\n",
        "                              verbose=1,  # Verbosity mode (1: progress bar, 0: silent)\n",
        "                              patience=6,  # Number of epochs with no improvement after which training will be stopped\n",
        "                              mode='min',  # Direction of improvement (minimize training loss)\n",
        "                              restore_best_weights=True  # Restore the model weights from the epoch with the best training loss\n",
        "                          )\n",
        "                         )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "FAvSYBCta0Xh"
      },
      "outputs": [],
      "source": [
        "# Extract the loss and validation loss from the training history\n",
        "loss = history.history['loss']\n",
        "\n",
        "# Plot the loss and validation loss\n",
        "epochs = range(1, len(loss) + 1)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(epochs, loss, 'b', label='Training loss')  # Plot training loss\n",
        "plt.xlabel('Epochs')  # Set x-axis label\n",
        "plt.ylabel('Loss')  # Set y-axis label\n",
        "plt.legend()  # Display legend\n",
        "plt.show()  # Show plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "eIlfDFvnhJby"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Predict on anomaly images\n",
        "reconstructed_anomalies = autoencoder.predict(anomaly_images)\n",
        "\n",
        "# Calculate reconstruction error\n",
        "reconstruction_errors = calculate_reconstruction_error(anomaly_images, reconstructed_anomalies)\n",
        "\n",
        "# Threshold for anomaly detection (this can be tuned)\n",
        "threshold = np.percentile(reconstruction_errors, 95)\n",
        "\n",
        "# Detect anomalies\n",
        "anomalies_detected = reconstruction_errors > threshold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "fRT5ToRDjFA1"
      },
      "outputs": [],
      "source": [
        "# Predict on both good and anomaly images\n",
        "reconstructed_good = autoencoder.predict(normal_images)\n",
        "reconstructed_anomalies = autoencoder.predict(anomaly_images)\n",
        "\n",
        "# Calculate reconstruction error for both\n",
        "good_errors = calculate_reconstruction_error(normal_images, reconstructed_good)\n",
        "anomaly_errors = calculate_reconstruction_error(anomaly_images, reconstructed_anomalies)\n",
        "\n",
        "# Combine errors and labels\n",
        "all_errors = np.concatenate([good_errors, anomaly_errors])\n",
        "all_labels = np.concatenate([np.zeros(len(good_errors)), np.ones(len(anomaly_errors))])\n",
        "\n",
        "# Compute AUROC\n",
        "auroc = roc_auc_score(all_labels, all_errors)\n",
        "print(f'AUROC: {auroc:.4f}')\n",
        "\n",
        "# Compute Average Precision (AP)\n",
        "ap = average_precision_score(all_labels, all_errors)\n",
        "print(f'Average Precision (AP): {ap:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p2jsHa6Al9yQ"
      },
      "outputs": [],
      "source": [
        "# Predict on both good and anomaly images\n",
        "reconstructed_normal = autoencoder.predict(normal_images)\n",
        "reconstructed_anomalies = autoencoder.predict(anomaly_images)\n",
        "\n",
        "# Calculate reconstruction error\n",
        "normal_reconstruction_errors = calculate_reconstruction_error(normal_images, reconstructed_normal)\n",
        "anomaly_reconstruction_errors = calculate_reconstruction_error(anomaly_images, reconstructed_anomalies)\n",
        "reconstruction_errors = (np.mean(anomaly_reconstruction_errors) + np.mean(normal_reconstruction_errors))/2\n",
        "\n",
        "all_errors = np.concatenate([normal_reconstruction_errors, anomaly_reconstruction_errors])\n",
        "all_labels = np.concatenate([np.zeros(len(good_errors)), np.ones(len(anomaly_errors))])\n",
        "\n",
        "# Threshold for anomaly detection (this can be tuned)\n",
        "threshold = np.percentile(reconstruction_errors, 20)\n",
        "# threshold = np.mean(reconstruction_errors)\n",
        "\n",
        "predicted_labels = (all_errors > threshold).astype(int)\n",
        "# Detect anomalies\n",
        "\n",
        "# Compute AUROC\n",
        "auroc = roc_auc_score(all_labels, predicted_labels)\n",
        "print(f'AUROC: {auroc:.4f}')\n",
        "\n",
        "# Compute Average Precision (AP)\n",
        "ap = average_precision_score(all_labels, predicted_labels)\n",
        "print(f'Average Precision (AP): {ap:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M15UjkpMvCJe"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jk26mZENyIZx"
      },
      "source": [
        "#One-class SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VCh14J4GyNDZ",
        "outputId": "2575da9e-d6f8-4946-969b-9ec2e5154249"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AUC-ROC: 0.9522292993630573\n",
            "Average Precision (AP): 0.9806007307058101\n"
          ]
        }
      ],
      "source": [
        "# Flatten the images\n",
        "X_flat = normal_images.reshape(normal_images.shape[0], -1)\n",
        "# Scale the data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X_flat)\n",
        "# Train One-Class SVM\n",
        "ocsvm = OneClassSVM(kernel='rbf', nu=0.1)  # Adjust parameters as needed\n",
        "ocsvm.fit(X_scaled)\n",
        "\n",
        "anomaly_scaled = anomaly_images.reshape(anomaly_images.shape[0], -1)\n",
        "anomaly_scaled = scaler.transform(anomaly_scaled)\n",
        "\n",
        "\n",
        "# Predict images\n",
        "all_images = np.concatenate((X_scaled, anomaly_scaled), axis=0)\n",
        "all_labels = np.concatenate((np.ones(X_scaled.shape[0]) , -1*np.ones(anomaly_scaled.shape[0])))\n",
        "\n",
        "all_prediction = ocsvm.predict(all_images)\n",
        "\n",
        "auc_roc = roc_auc_score(all_labels, all_prediction)\n",
        "print(f\"AUC-ROC: {auc_roc}\")\n",
        "\n",
        "# Calculate Average Precision (AP)\n",
        "average_precision = average_precision_score(all_labels, all_prediction)\n",
        "print(f\"Average Precision (AP): {average_precision}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlnFlW0Szi95"
      },
      "source": [
        "#Isonation Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZgBFenzzU0d"
      },
      "outputs": [],
      "source": [
        "X_good = flatten_images(normal_images)\n",
        "X_anomaly = flatten_images(anomaly_images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "abhMCExV0UhD"
      },
      "outputs": [],
      "source": [
        "# Scale the data\n",
        "scaler = StandardScaler()\n",
        "X_good = scaler.fit_transform(X_good)\n",
        "X_anomaly = scaler.transform(X_anomaly)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "orQnHUUx0aZy"
      },
      "outputs": [],
      "source": [
        "# Define the parameter grid\n",
        "# param_grid = {\n",
        "#     'n_estimators': [100, 200, 300],\n",
        "#     'max_samples': [128, 256, 512, 'auto'],\n",
        "#     'contamination': [0.01, 0.05, 0.1],\n",
        "#     'max_features': [0.5, 0.75, 1.0],\n",
        "#     'bootstrap': [True, False]\n",
        "# }\n",
        "\n",
        "param_grid = {\n",
        "    'n_estimators': [100],\n",
        "    'max_samples': ['auto'],\n",
        "    'contamination': [0.01, 0.05, 0.1],\n",
        "    'max_features': [0.5, 0.75, 1.0],\n",
        "    'bootstrap': [False]\n",
        "}\n",
        "\n",
        "# Initialize the Isolation Forest model\n",
        "isolation_forest = IsolationForest(random_state=42)\n",
        "\n",
        "# Initialize the GridSearchCV object\n",
        "grid_search = GridSearchCV(estimator=isolation_forest, param_grid=param_grid, scoring='roc_auc', cv=5, n_jobs=-1)\n",
        "\n",
        "grid_search.fit(X_good)\n",
        "\n",
        "# Print the best parameters and the corresponding score\n",
        "print(\"Best parameters found: \", grid_search.best_params_)\n",
        "print(\"Best AUCROC score: \", grid_search.best_score_)\n",
        "\n",
        "\n",
        "all_images = np.concatenate((X_good, X_anomaly), axis=0)\n",
        "all_labels = np.concatenate((np.ones(X_good.shape[0]) , -1*np.ones(X_anomaly.shape[0])))\n",
        "\n",
        "predictions = isolation_forest.predict(all_images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wh8E2H-x0hDs"
      },
      "outputs": [],
      "source": [
        "# Compute AUCROC\n",
        "aucroc = roc_auc_score(predictions, all_labels)\n",
        "\n",
        "# Compute AP\n",
        "ap = average_precision_score(predictions, all_labels)\n",
        "\n",
        "print(f'AUCROC: {aucroc}')\n",
        "print(f'AP: {ap}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dr67eG2J0maF"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "jk26mZENyIZx"
      ],
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyMHC7OqOqEU+evrwRgJ2pgn",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}